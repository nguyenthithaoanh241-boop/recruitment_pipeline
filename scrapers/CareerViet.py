# scrapers/careerviet_scraper.py

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import time, random, csv, os, datetime, re

class CareerVietScraper:
    def __init__(self, category_name, base_url):
        """Kh·ªüi t·∫°o scraper cho m·ªôt danh m·ª•c c·ª• th·ªÉ tr√™n CareerViet."""
        self.category_name = category_name
        self.base_url = base_url
        self.SOURCE_WEB = "CareerViet"

        # ===== C·∫§U H√åNH CHUNG =====
        self.PAUSE_BETWEEN_PAGES_MIN = 3
        self.PAUSE_BETWEEN_PAGES_MAX = 6
        self.PAUSE_BETWEEN_JOBS_MIN = 4
        self.PAUSE_BETWEEN_JOBS_MAX = 8
        self.JOBS_PER_LONG_BREAK = 50
        self.LONG_BREAK_DURATION_MIN = 60
        self.LONG_BREAK_DURATION_MAX = 120

        # ===== C·∫§U H√åNH QU√âT TRANG (GI·ªêNG TOPCV) =====
        self.START_PAGE = 1
        self.PAGES_TO_ADD_PER_RUN = 2 # S·ªë trang s·∫Ω c·ªông th√™m cho l·∫ßn ch·∫°y k·∫ø ti·∫øp

        # ===== THI·∫æT L·∫¨P ƒê∆Ø·ªúNG D·∫™N =====
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        scraper_dir = os.path.dirname(os.path.abspath(__file__))

        self.csv_output_dir = os.path.join(project_root, "dataset")
        os.makedirs(self.csv_output_dir, exist_ok=True)
        
        self.log_file = os.path.join(scraper_dir, "CareerViet_log.txt")
        self.id_history_file = os.path.join(scraper_dir, "CareerViet_id_history.txt")
        # File ƒë·ªÉ l∆∞u trang t·ªëi ƒëa ƒë√£ qu√©t
        self.max_page_file = os.path.join(scraper_dir, f"CareerViet_{self.category_name}_max_page.txt")


        self.CSV_HEADER = [
            "title", "work_location", "salary", "experience", "level", "work_form", "company_name", "company_link",
            "company_size", "gender", "education", "age", "careers_field", "skills", "job_description","requirement", "benefits",
            "post_date", "deadline", "link", "source_web"
        ]

    def _create_driver(self):
        """T·∫°o v√† tr·∫£ v·ªÅ m·ªôt instance c·ªßa Chrome WebDriver."""
        chrome_options = Options()
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--disable-infobars")
        chrome_options.add_argument("--start-maximized")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        # chrome_options.add_argument("--headless=new")
        return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

    def _write_log(self, message):
        """Ghi log c√≥ ƒë·ªãnh d·∫°ng."""
        now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{now}] [{self.category_name}] {message}\n"
        with open(self.log_file, "a", encoding="utf-8") as log:
            log.write(log_message)
        print(log_message.strip())

    def _get_existing_ids(self):
        """ƒê·ªçc v√† tr·∫£ v·ªÅ m·ªôt set c√°c ID ƒë√£ c√†o t·ª´ tr∆∞·ªõc."""
        if not os.path.exists(self.id_history_file): return set()
        try:
            with open(self.id_history_file, 'r', encoding='utf-8') as f:
                return {line.strip() for line in f if line.strip()}
        except Exception as e:
            self._write_log(f"L·ªói khi ƒë·ªçc file l·ªãch s·ª≠ ID: {e}")
            return set()

    def _extract_job_id_from_link(self, link):
        """Tr√≠ch xu·∫•t ID t·ª´ link job c·ªßa CareerViet."""
        if not link: return None
        match = re.search(r'-(\w+)\.html', link)
        return match.group(1) if match else None

    def _human_like_scroll(self, driver):
        """Cu·ªôn trang m·ªôt c√°ch t·ª± nhi√™n."""
        try:
            scroll_height = driver.execute_script("return document.body.scrollHeight")
            for i in range(0, scroll_height, random.randint(300, 500)):
                driver.execute_script(f"window.scrollTo(0, {i});")
                time.sleep(random.uniform(0.3, 0.7))
        except Exception as e:
            self._write_log(f"L·ªói khi cu·ªôn trang: {e}")

    # --- Helper methods for scraping details ---
    def _get_text_by_label(self, driver, label):
        try:
            xpath = f"//strong[contains(., '{label}')]/following-sibling::p"
            info = driver.find_element(By.XPATH, xpath).text
            return " ".join(info.split())
        except NoSuchElementException:
            return ""

    def _get_other_info(self, driver, label):
        try:
            xpath = f"//h3[text()='Th√¥ng tin kh√°c']/following-sibling::div//li[contains(., '{label}')]"
            li_element = driver.find_element(By.XPATH, xpath)
            full_text = li_element.text
            value = full_text.replace(label, '').replace(':', '').strip()
            return value
        except NoSuchElementException:
            return ""
            
    def run(self):
        """Ph∆∞∆°ng th·ª©c ch√≠nh ƒë·ªÉ ch·∫°y to√†n b·ªô qu√° tr√¨nh c√†o d·ªØ li·ªáu."""
        self._write_log("üöÄ B·∫Øt ƒë·∫ßu phi√™n c√†o d·ªØ li·ªáu CareerViet m·ªõi...")
        
        now_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        output_file = os.path.join(self.csv_output_dir, f"CareerViet_{self.category_name}_jobs_{now_str}.csv")
        self._write_log(f"üìÑ D·ªØ li·ªáu l·∫ßn n√†y s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o file: {os.path.basename(output_file)}")
        
        with open(output_file, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(self.CSV_HEADER)

        # ===== LOGIC ƒê·ªåC/GHI S·ªê TRANG T·ªêI ƒêA =====
        try:
            if not os.path.exists(self.max_page_file):
                max_page_to_crawl = self.PAGES_TO_ADD_PER_RUN
                with open(self.max_page_file, 'w') as f: f.write(str(max_page_to_crawl))
                self._write_log(f"File max_page.txt kh√¥ng t·ªìn t·∫°i. T·∫°o m·ªõi v√† ƒë·∫∑t trang t·ªëi ƒëa l√† {max_page_to_crawl}.")
            else:
                with open(self.max_page_file, 'r') as f:
                    content = f.readline().strip()
                    if content and content.isdigit():
                        max_page_to_crawl = int(content)
                    else:
                        max_page_to_crawl = self.PAGES_TO_ADD_PER_RUN
                        self._write_log(f"N·ªôi dung file max_page.txt kh√¥ng h·ª£p l·ªá. ƒê·∫∑t l·∫°i trang t·ªëi ƒëa l√† {max_page_to_crawl}.")
        except Exception as e:
            max_page_to_crawl = self.PAGES_TO_ADD_PER_RUN
            self._write_log(f"L·ªói khi ƒë·ªçc file max_page.txt: {e}. ƒê·∫∑t l·∫°i trang t·ªëi ƒëa l√† {max_page_to_crawl}.")

        self._write_log(f"üìå L·∫ßn n√†y s·∫Ω qu√©t t·ª´ trang {self.START_PAGE} ‚Üí {max_page_to_crawl}.")
        
        driver = self._create_driver()
        existing_ids = self._get_existing_ids()
        self._write_log(f"üìä ƒê√£ t√¨m th·∫•y {len(existing_ids)} ID jobs trong l·ªãch s·ª≠ chung c·ªßa CareerViet.")
        
        # --- Thu th·∫≠p link c·ªßa c√°c job m·ªõi ---
        new_jobs_to_crawl = []
        for page in range(self.START_PAGE, max_page_to_crawl + 1):
            base_link_part = self.base_url.split('.html')[0]
            url = f"{base_link_part}-trang-{page}-vi.html"
            
            self._write_log(f"üîé ƒêang qu√©t trang {page}/{max_page_to_crawl}: {url}")
            try:
                driver.get(url)
                self._human_like_scroll(driver)
                WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.job-item has-background")))
            except TimeoutException:
                 self._write_log(f"‚ö†Ô∏è Trang {page} kh√¥ng load ƒë∆∞·ª£c ho·∫∑c kh√¥ng c√≥ job (h·∫øt trang). D·ª´ng qu√©t link.")
                 break # D·ª´ng v√≤ng l·∫∑p n·∫øu h·∫øt trang
            except Exception as e:
                self._write_log(f"‚ö†Ô∏è L·ªói khi t·∫£i trang {page}. B·ªè qua. L·ªói: {e}")
                continue

            job_cards = driver.find_elements(By.CSS_SELECTOR, "div.job-item.has-background")
            
            new_jobs_found_on_page = 0
            # 2. L·∫∑p qua t·ª´ng card
            for card in job_cards:
                try:
                    # 3. T√¨m link b√™n trong card v√† l·∫•y href
                    link_element = card.find_element(By.CSS_SELECTOR, "a.job_link")
                    link_job = link_element.get_attribute("href")
                    
                    job_id = self._extract_job_id_from_link(link_job)
                    if job_id and job_id not in existing_ids:
                        new_jobs_to_crawl.append((link_job, job_id))
                        existing_ids.add(job_id)
                        new_jobs_found_on_page += 1
                except Exception:
                    continue
            
            if new_jobs_found_on_page > 0:
                self._write_log(f"Trang {page} ‚Üí T√¨m th·∫•y {new_jobs_found_on_page} job M·ªöI.")
            else:
                self._write_log(f"Trang {page} kh√¥ng c√≥ job n√†o m·ªõi.")
            
            time.sleep(random.uniform(self.PAUSE_BETWEEN_PAGES_MIN, self.PAUSE_BETWEEN_PAGES_MAX))

        self._write_log(f"üéâ ƒê√£ thu th·∫≠p xong. C√≥ {len(new_jobs_to_crawl)} job m·ªõi c·∫ßn c√†o chi ti·∫øt.")
        
        # --- C√†o chi ti·∫øt t·ª´ng job ---
        success_count, error_count = 0, 0
        if not new_jobs_to_crawl:
            self._write_log("Kh√¥ng c√≥ job m·ªõi n√†o ƒë·ªÉ c√†o. K·∫øt th√∫c.")
        else:
            for idx, (link, job_id) in enumerate(new_jobs_to_crawl, 1):
                try:
                    driver.get(link)
                    wait = WebDriverWait(driver, 20)
                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.job-detail")))
                    self._human_like_scroll(driver)
                    
                    # D·ªçn d·∫πp logic c√†o chi ti·∫øt
                    title = driver.find_element(By.CSS_SELECTOR, "h1.title").text.strip()
                    location_div = wait.until(EC.visibility_of_element_located((By.XPATH, "//h3[text()='ƒê·ªãa ƒëi·ªÉm l√†m vi·ªác']/following-sibling::div")))
                    work_location = ", ".join(location_div.text.split('\n'))

                    salary = self._get_text_by_label(driver, "L∆∞∆°ng")
                    level = self._get_text_by_label(driver, "C·∫•p b·∫≠c")
                    deadline = self._get_text_by_label(driver, "H·∫øt h·∫°n n·ªôp")
                    experience = self._get_text_by_label(driver, "Kinh nghi·ªám")
                    post_date = self._get_text_by_label(driver, "Ng√†y c·∫≠p nh·∫≠t")
                    work_form = self._get_text_by_label(driver, "H√¨nh th·ª©c")

                    education = self._get_other_info(driver, "B·∫±ng c·∫•p")
                    gender = self._get_other_info(driver, "Gi·ªõi t√≠nh")
                    age = self._get_other_info(driver, "ƒê·ªô tu·ªïi")

                    # L·∫•y m√¥ t·∫£, y√™u c·∫ßu, quy·ªÅn l·ª£i
                    desc_elements = driver.find_elements(By.XPATH, "//h2[text()='M√¥ t·∫£ C√¥ng vi·ªác']/following-sibling::*")
                    job_description = "\n".join([el.text for el in desc_elements if el.tag_name == 'p'])
                    
                    req_elements = driver.find_elements(By.XPATH, "//h2[text()='Y√™u C·∫ßu C√¥ng Vi·ªác']/following-sibling::p[count(preceding-sibling::p/strong[contains(.,'QUY·ªÄN L·ª¢I')])=0]")
                    requirement = "\n".join([p.text for p in req_elements if p.text.strip()])
                    
                    benefit_elements = driver.find_elements(By.XPATH, "//p[strong[contains(.,'QUY·ªÄN L·ª¢I')]]/following-sibling::p[count(following-sibling::p/strong[contains(.,'PH√öC L·ª¢I')]) > 0]")
                    benefits = "\n".join([p.text for p in benefit_elements if p.text.strip()])

                    # L·∫•y careers v√† skills
                    career_elements = driver.find_elements(By.XPATH, "//strong[contains(., 'Ng√†nh ngh·ªÅ')]/following-sibling::p//a")
                    careers_list = [" ".join(c.text.split()) for c in career_elements if c.text.strip()]
                    careers_field = ", ".join(careers_list)

                    skill_elements = driver.find_elements(By.CSS_SELECTOR, "div.job-tags a")
                    skills_list = [skill.text.strip() for skill in skill_elements if skill.text.strip()]
                    skills_str = ", ".join(skills_list)
                    
                    # L·∫•y th√¥ng tin c√¥ng ty
                    company_name, company_link, company_size = "", "", ""
                    try:
                        company_tab = wait.until(EC.element_to_be_clickable((By.ID, "tabs-job-company")))
                        company_tab.click()
                        time.sleep(1) # Ch·ªù tab load
                        comp_element = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "div.title-company a.name")))
                        company_name = comp_element.text
                        company_link = comp_element.get_attribute('href')
                        size_element = driver.find_element(By.XPATH, "//li[contains(., 'Quy m√¥ c√¥ng ty')]")
                        company_size = size_element.text.split(':')[-1].strip()
                    except Exception:
                        self._write_log(f"Kh√¥ng t√¨m th·∫•y th√¥ng tin c√¥ng ty cho job ID {job_id}")

                    # Ghi v√†o CSV
                    with open(output_file, "a", encoding="utf-8-sig", newline="") as f:
                        writer = csv.writer(f)
                        writer.writerow([
                            title, work_location, salary, experience, level, work_form,
                            company_name, company_link, company_size, gender, education, age,
                            careers_field, skills_str, job_description,requirement, benefits, post_date,
                            deadline, link, self.SOURCE_WEB])
                    
                    with open(self.id_history_file, "a", encoding="utf-8") as f:
                        f.write(job_id + "\n")

                    success_count += 1
                    self._write_log(f"‚úÖ [{success_count}/{len(new_jobs_to_crawl)}] ƒê√£ c√†o v√† l∆∞u job ID {job_id}: {title[:60]}...")
                    
                    # T·∫°m ngh·ªâ
                    if success_count % self.JOBS_PER_LONG_BREAK == 0 and success_count < len(new_jobs_to_crawl):
                        sleep_time = random.uniform(self.LONG_BREAK_DURATION_MIN, self.LONG_BREAK_DURATION_MAX)
                        self._write_log(f"‚è∏ Ngh·ªâ d√†i sau {success_count} job... S·∫Ω ti·∫øp t·ª•c sau {round(sleep_time/60, 1)} ph√∫t.")
                        time.sleep(sleep_time)
                    else:
                        time.sleep(random.uniform(self.PAUSE_BETWEEN_JOBS_MIN, self.PAUSE_BETWEEN_JOBS_MAX))
                
                except Exception as e:
                    error_count += 1
                    self._write_log(f"‚ùå L·ªói khi x·ª≠ l√Ω link {idx}/{len(new_jobs_to_crawl)} (ID: {job_id}): {link} | {e}")
                    continue
        
        # ===== C·∫¨P NH·∫¨T FILE MAX_PAGE CHO L·∫¶N CH·∫†Y TI·∫æP THEO =====
        new_max_page = max_page_to_crawl + self.PAGES_TO_ADD_PER_RUN
        try:
            with open(self.max_page_file, "w") as f: f.write(str(new_max_page))
            self._write_log(f"üîÑ ƒê√£ c·∫≠p nh·∫≠t max_page.txt cho l·∫ßn ch·∫°y ti·∫øp theo: {new_max_page}")
        except Exception as e:
            self._write_log(f"‚ùå Kh√¥ng th·ªÉ c·∫≠p nh·∫≠t file max_page.txt: {e}")

        driver.quit()
        self._write_log(f"üèÅ Crawl xong - ƒê√£ l∆∞u {success_count} job M·ªöI, L·ªói: {error_count}")