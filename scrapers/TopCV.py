# scrapers/topcv_scraper.py

import selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time, random, csv, os, datetime, sys, re
import logging 

# <--- TH√äM M·ªöI: Import h√†m loader t·ª´ file script/loader.py
# (Gi·∫£ s·ª≠ file script/ n·∫±m c√πng c·∫•p v·ªõi th∆∞ m·ª•c scrapers/ trong project_root)
project_root_for_import = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root_for_import)
from pipeline.loader import load_csv_to_staging_and_cleanup

class TopCVScraper:
    def __init__(self):
        """Kh·ªüi t·∫°o c√°c bi·∫øn c·∫•u h√¨nh v√† ƒë∆∞·ªùng d·∫´n cho scraper TopCV."""
        # ===== C·∫§U H√åNH CHO PIPELINE =====
        self.START_PAGE = 1
        self.PAGES_TO_ADD_PER_RUN = 1 # S·ªë trang s·∫Ω c·ªông th√™m cho l·∫ßn ch·∫°y k·∫ø ti·∫øp
        self.JOBS_PER_BREAK = 50
        self.BREAK_DURATION_MIN = 120
        self.BREAK_DURATION_MAX = 300
        self.BATCH_SIZE_RESTART_DRIVER = 50
        self.SOURCE_WEB = "TopCV"
        
        # ===== THI·∫æT L·∫¨P ƒê∆Ø·ªúNG D·∫™N =====
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        scraper_dir = os.path.dirname(os.path.abspath(__file__))

        self.csv_output_dir = os.path.join(project_root, "dataset")
        os.makedirs(self.csv_output_dir, exist_ok=True)

        self.log_file = os.path.join(scraper_dir, "TopCV.log") # <--- S·ª¨A: ƒê·ªïi .txt th√†nh .log
        self.id_history_file = os.path.join(scraper_dir, "TopCV_id_history.txt")
        self.max_page_file = os.path.join(scraper_dir, "TopCV_max_page.txt")

        # <--- THAY ƒê·ªîI: Th√™m 2 c·ªôt m·ªõi v√†o Header
        self.CSV_HEADER = [
            "title", "specialization", "work_location", "experience", "salary",
            "work_time", "level", "work_form", "company_name", "company_link",
            "company_size", "recruit_quantity", "education",
            "requirement", "job_description", "benefits", "deadline", "link", "source_web",
            "scraped_at"
        ]
        
        # <--- TH√äM M·ªöI: Thi·∫øt l·∫≠p logger
        self._setup_logging()
        self.logger = logging.getLogger(self.SOURCE_WEB) # <--- Logger ri√™ng cho TopCV

    def _setup_logging(self): # <--- TH√äM M·ªöI: H√†m thi·∫øt l·∫≠p logging
        """C·∫•u h√¨nh logging ƒë·ªÉ ghi ra file v√† console."""
        logger = logging.getLogger(self.SOURCE_WEB)
        logger.setLevel(logging.INFO) # Ch·ªâ log t·ª´ m·ª©c INFO tr·ªü l√™n

        # B·ªè c√°c handler c≈© n·∫øu ƒë√£ t·ªìn t·∫°i
        if logger.hasHandlers():
            logger.handlers.clear()

        # ƒê·ªãnh d·∫°ng log
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

        # Handler cho File
        file_handler = logging.FileHandler(self.log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

        # Handler cho Console
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

    def _create_driver(self):
        """T·∫°o v√† tr·∫£ v·ªÅ m·ªôt instance c·ªßa Chrome WebDriver."""
        chrome_options = Options()
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--disable-infobars")
        chrome_options.add_argument("--start-maximized")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        #chrome_options.add_argument("--headless=new") # <--- B·ªè comment n·∫øu ch·∫°y tr√™n server
        return webdriver.Chrome(options=chrome_options)

    

    def _get_existing_ids(self, file_path):
        """ƒê·ªçc v√† tr·∫£ v·ªÅ m·ªôt set c√°c ID ƒë√£ c√†o t·ª´ tr∆∞·ªõc."""
        if not os.path.exists(file_path):
            return set()
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return {line.strip() for line in f if line.strip()}
        except Exception as e:
            self.logger.error(f"L·ªói khi ƒë·ªçc file l·ªãch s·ª≠ ID {file_path}: {e}") # <--- S·ª¨A: D√πng logger
            return set()

    def _extract_job_id_from_link(self, link):
        """Tr√≠ch xu·∫•t ID t·ª´ link job c·ªßa TopCV."""
        if not link:
            return None
        match = re.search(r'/(\d+)\.html', link)
        return match.group(1) if match else None
        
    def _get_element_text(self, driver, by, value):
        """Helper function ƒë·ªÉ l·∫•y text c·ªßa element m·ªôt c√°ch an to√†n."""
        try:
            return driver.find_element(by, value).text.strip()
        except NoSuchElementException:
            return ""

    def _get_section_details(self, driver, section_title):
        """L·∫•y n·ªôi dung chi ti·∫øt c·ªßa m·ªôt m·ª•c trong m√¥ t·∫£ c√¥ng vi·ªác."""
        try:
            section_elements = driver.find_elements(By.XPATH, f"//h3[contains(text(),'{section_title}')]/following-sibling::div[@class='job-description__item--content']//*")
            texts = [el.text.strip() for el in section_elements if el.text.strip()]
            return ". ".join(texts)
        except NoSuchElementException:
            return ""

    def run(self):
        """Ph∆∞∆°ng th·ª©c ch√≠nh ƒë·ªÉ ch·∫°y to√†n b·ªô qu√° tr√¨nh c√†o d·ªØ li·ªáu."""
        start_time = time.time()
        self.logger.info("üöÄ B·∫Øt ƒë·∫ßu phi√™n c√†o d·ªØ li·ªáu TopCV m·ªõi...") # <--- S·ª¨A: D√πng logger

        now_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        output_file = os.path.join(self.csv_output_dir, f"TopCV_jobs_{now_str}.csv")
        self.logger.info(f"üìÑ D·ªØ li·ªáu l·∫ßn n√†y s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o file: {os.path.basename(output_file)}") # <--- S·ª¨A

        with open(output_file, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(self.CSV_HEADER)
            
        try:
            if not os.path.exists(self.max_page_file):
                max_page_to_crawl = self.PAGES_TO_ADD_PER_RUN
                with open(self.max_page_file, 'w') as f: f.write(str(max_page_to_crawl))
                self.logger.info(f"File max_page.txt kh√¥ng t·ªìn t·∫°i. T·∫°o m·ªõi v√† ƒë·∫∑t trang t·ªëi ƒëa l√† {max_page_to_crawl}.") # <--- S·ª¨A
            else:
                with open(self.max_page_file, 'r') as f:
                    content = f.readline().strip()
                    if content and content.isdigit():
                        max_page_to_crawl = int(content)
                    else:
                        max_page_to_crawl = self.PAGES_TO_ADD_PER_RUN
                        self.logger.warning(f"N·ªôi dung file max_page.txt kh√¥ng h·ª£p l·ªá. ƒê·∫∑t l·∫°i trang t·ªëi ƒëa l√† {max_page_to_crawl}.") # <--- S·ª¨A
        except Exception as e:
            max_page_to_crawl = self.PAGES_TO_ADD_PER_RUN
            self.logger.error(f"L·ªói khi ƒë·ªçc file max_page.txt: {e}. ƒê·∫∑t l·∫°i trang t·ªëi ƒëa l√† {max_page_to_crawl}.") # <--- S·ª¨A

        self.logger.info(f"üìå L·∫ßn n√†y s·∫Ω qu√©t to√†n b·ªô t·ª´ trang {self.START_PAGE} ‚Üí {max_page_to_crawl}.") # <--- S·ª¨A
        
        driver = self._create_driver()
        existing_ids = self._get_existing_ids(self.id_history_file)
        self.logger.info(f"üìä ƒê√£ t√¨m th·∫•y {len(existing_ids)} ID jobs trong l·ªãch s·ª≠.") # <--- S·ª¨A

        new_jobs_to_crawl = []
        
        for page in range(self.START_PAGE, max_page_to_crawl + 1):
            url = f"https://www.topcv.vn/tim-viec-lam-cong-nghe-thong-tin-cr257?sort=newp&type_keyword={page}&category_family=r257"
            
            self.logger.info(f"üîé ƒêang qu√©t trang {page}: {url}") # <--- S·ª¨A
            try:
                driver.get(url)
                WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.job-item-search-result")))
                time.sleep(random.uniform(2, 4))
            except TimeoutException:
                self.logger.warning(f"Trang {page} kh√¥ng t·ªìn t·∫°i ho·∫∑c load qu√° l√¢u. B·ªè qua.") # <--- S·ª¨A
                continue
            
            job_cards = driver.find_elements(By.CSS_SELECTOR, "div.job-item-search-result")
            if not job_cards:
                self.logger.warning(f"‚ö†Ô∏è Trang {page} kh√¥ng c√≥ job n√†o. Ti·∫øp t·ª•c qu√©t trang ti·∫øp theo.") # <--- S·ª¨A
                continue

            new_jobs_found_on_page = 0
            for card in job_cards:
                try:
                    link_element = card.find_element(By.CSS_SELECTOR, "h3.title a")
                    link = link_element.get_attribute("href")
                    job_id = self._extract_job_id_from_link(link)
                    if job_id and job_id not in existing_ids:
                        new_jobs_to_crawl.append((link, job_id))
                        existing_ids.add(job_id)
                        new_jobs_found_on_page += 1
                except Exception:
                    continue
            
            if new_jobs_found_on_page > 0:
                self.logger.info(f"Trang {page} ‚Üí T√¨m th·∫•y {new_jobs_found_on_page} job M·ªöI.") # <--- S·ª¨A
            else:
                self.logger.info(f"Trang {page} kh√¥ng c√≥ job n√†o m·ªõi.") # <--- S·ª¨A

        self.logger.info(f"üéâ ƒê√£ thu th·∫≠p xong. C√≥ {len(new_jobs_to_crawl)} job m·ªõi c·∫ßn c√†o chi ti·∫øt.") # <--- S·ª¨A

        success_count, error_count = 0, 0
        if not new_jobs_to_crawl:
            self.logger.info("Kh√¥ng c√≥ job m·ªõi n√†o ƒë·ªÉ c√†o. K·∫øt th√∫c.") # <--- S·ª¨A
        else:
            for idx, (link, job_id) in enumerate(new_jobs_to_crawl, 1):
                try:
                    driver.get(link)
                    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.job-detail__body")))
                    time.sleep(random.uniform(2, 5))
                    
                    # <--- TH√äM M·ªöI: L·∫•y th·ªùi gian c√†o ngay t·∫°i th·ªùi ƒëi·ªÉm n√†y
                    scraped_timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                    title = self._get_element_text(driver, By.CSS_SELECTOR, "h1.job-detail__info--title")
                    salary = self._get_element_text(driver, By.XPATH, "//div[div[contains(text(), 'M·ª©c l∆∞∆°ng')]]/div[contains(@class, 'value')]")
                    experience = self._get_element_text(driver, By.XPATH, "//div[div[contains(text(), 'Kinh nghi·ªám')]]/div[contains(@class, 'value')]")
                    level = self._get_element_text(driver, By.XPATH, "//div[div[contains(text(), 'C·∫•p b·∫≠c')]]/div[contains(@class, 'value')]")
                    recruit_quantity = self._get_element_text(driver, By.XPATH, "//div[div[contains(text(), 'S·ªë l∆∞·ª£ng tuy·ªÉn')]]/div[contains(@class, 'value')]")
                    work_form = self._get_element_text(driver, By.XPATH, "//div[div[contains(text(), 'H√¨nh th·ª©c l√†m vi·ªác')]]/div[contains(@class, 'value')]")
                    education = self._get_element_text(driver, By.XPATH, "//div[div[contains(text(), 'H·ªçc v·∫•n')]]/div[contains(@class, 'value')]")
                    deadline_raw = self._get_element_text(driver, By.CSS_SELECTOR, "div.job-detail__info--deadline")

                    specialization = self._get_element_text(driver, By.CSS_SELECTOR, "a.item.search-from-tag.link")
                    work_location = self._get_element_text(driver, By.XPATH, "//h3[contains(text(),'ƒê·ªãa ƒëi·ªÉm l√†m vi·ªác')]/following-sibling::div")
                    work_time = self._get_element_text(driver, By.XPATH,"//h3[contains(text(),'Th·ªùi gian l√†m vi·ªác')]/following-sibling::div")
                    
                    company_name = self._get_element_text(driver, By.CSS_SELECTOR, "a.name")
                    company_link = driver.find_element(By.CSS_SELECTOR, ".job-detail__box--right.job-detail__company a").get_attribute("href") if company_name else ""
                    company_size = self._get_element_text(driver, By.XPATH, "//div[contains(@class, 'company-scale')]//div[@class='company-value']")
                    
                    job_description = self._get_section_details(driver, "M√¥ t·∫£ c√¥ng vi·ªác")
                    requirement = self._get_section_details(driver, "Y√™u c·∫ßu ·ª©ng vi√™n")
                    benefits = self._get_section_details(driver, "Quy·ªÅn l·ª£i")

                    # <--- S·ª¨A: Th√™m 2 c·ªôt m·ªõi v√†o d·ªØ li·ªáu
                    job_data = [
                        title, specialization, work_location, experience, salary, work_time, level, work_form,
                        company_name, company_link, company_size, recruit_quantity, education, requirement, job_description, benefits,
                        deadline_raw.replace('H·∫°n n·ªôp h·ªì s∆°: ', ''), link, self.SOURCE_WEB,
                        scraped_timestamp # <--- 2 c·ªôt m·ªõi (th·ªùi gian c√†o, tr·∫°ng th√°i transform = 0)
                    ]
                    
                    with open(output_file, "a", encoding="utf-8-sig", newline="") as f:
                        writer = csv.writer(f)
                        writer.writerow(job_data)

                    with open(self.id_history_file, "a", encoding="utf-8") as f:
                        f.write(job_id + "\n")
                    
                    success_count += 1
                    self.logger.info(f"‚úÖ [{success_count}/{len(new_jobs_to_crawl)}] ƒê√£ c√†o v√† l∆∞u job ID {job_id}: {title}") # <--- S·ª¨A
                    
                    if success_count % self.JOBS_PER_BREAK == 0 and success_count < len(new_jobs_to_crawl):
                        pause_time = random.uniform(self.BREAK_DURATION_MIN, self.BREAK_DURATION_MAX)
                        self.logger.info(f"‚è∏ ƒê√£ c√†o {success_count} job. T·∫°m ngh·ªâ {round(pause_time/60, 1)} ph√∫t...") # <--- S·ª¨A
                        time.sleep(pause_time)
                    
                    if idx % self.BATCH_SIZE_RESTART_DRIVER == 0 and idx < len(new_jobs_to_crawl):
                        self.logger.info("üîÑ Kh·ªüi ƒë·ªông l·∫°i tr√¨nh duy·ªát...") # <--- S·ª¨A
                        driver.quit()
                        time.sleep(random.uniform(20, 40))
                        driver = self._create_driver()

                except Exception as e:
                    error_count += 1
                    self.logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω link {idx}/{len(new_jobs_to_crawl)} (ID: {job_id}): {link} | {e}") # <--- S·ª¨A
            
        driver.quit()

        # <--- TH√äM M·ªöI: Logic n·∫°p DB v√† d·ªçn d·∫πp file CSV ---
        if success_count > 0:
            self.logger.info(f"--- B·∫ÆT ƒê·∫¶U N·∫†P V√ÄO DATABASE ---")
            load_csv_to_staging_and_cleanup(output_file, schema='staging', table_name='raw_jobs')
            self.logger.info(f"--- K·∫æT TH√öC N·∫†P V√ÄO DATABASE ---")
        elif not new_jobs_to_crawl:
            self.logger.info("Kh√¥ng c√≥ job m·ªõi, kh√¥ng c·∫ßn n·∫°p v√†o DB.")
            try:
                os.remove(output_file) # X√≥a file CSV r·ªóng (ch·ªâ c√≥ header)
                self.logger.info(f"ƒê√£ x√≥a file CSV r·ªóng: {output_file}")
            except Exception as e:
                self.logger.error(f"Kh√¥ng th·ªÉ x√≥a file r·ªóng {output_file}: {e}")
        else: # C√≥ job m·ªõi nh∆∞ng c√†o l·ªói 100%
            self.logger.warning(f"T·∫•t c·∫£ {len(new_jobs_to_crawl)} job m·ªõi ƒë·ªÅu c√†o b·ªã l·ªói. Kh√¥ng n·∫°p v√†o DB.")
            try:
                os.remove(output_file) # X√≥a file CSV r·ªóng (ch·ªâ c√≥ header)
                self.logger.info(f"ƒê√£ x√≥a file CSV r·ªóng: {output_file}")
            except Exception as e:
                self.logger.error(f"Kh√¥ng th·ªÉ x√≥a file r·ªóng {output_file}: {e}")
        # --- H·∫øt kh·ªëi code th√™m m·ªõi ---

        new_max_page = max_page_to_crawl + self.PAGES_TO_ADD_PER_RUN
        try:
            with open(self.max_page_file, "w") as f: f.write(str(new_max_page))
            self.logger.info(f"üîÑ ƒê√£ c·∫≠p nh·∫≠t max_page.txt cho l·∫ßn ch·∫°y ti·∫øp theo: {new_max_page}") # <--- S·ª¨A
        except Exception as e:
            self.logger.error(f"‚ùå Kh√¥ng th·ªÉ c·∫≠p nh·∫≠t file max_page.txt: {e}") # <--- S·ª¨A

        end_time = time.time()
        total_minutes = round((end_time - start_time) / 60, 2)
        self.logger.info(f"üèÅ Crawl xong trong {total_minutes} ph√∫t - ƒê√£ l∆∞u {success_count} job M·ªöI, L·ªói: {error_count}") # <--- S·ª¨A