# scrapers/careerlink_scraper.py

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time, random, csv, os, datetime, re, sys, logging # <--- TH√äM 'sys' v√† 'logging'

# <--- TH√äM M·ªöI: Import h√†m loader t·ª´ file script/loader.py
project_root_for_import = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root_for_import)
from pipeline.loader import load_csv_to_staging_and_cleanup

class CareerLinkScraper:
    def __init__(self, category_name, base_url):
        """Kh·ªüi t·∫°o scraper cho m·ªôt danh m·ª•c c·ª• th·ªÉ tr√™n CareerLink."""
        self.category_name = category_name
        self.base_url = base_url
        self.SOURCE_WEB = "CareerLink"

        # ===== C·∫§U H√åNH CHUNG =====
        self.PAUSE_BETWEEN_PAGES_MIN = 3
        self.PAUSE_BETWEEN_PAGES_MAX = 6
        self.PAUSE_BETWEEN_JOBS_MIN = 4
        self.PAUSE_BETWEEN_JOBS_MAX = 8
        self.JOBS_PER_LONG_BREAK = 50
        self.LONG_BREAK_DURATION_MIN = 60
        self.LONG_BREAK_DURATION_MAX = 120
        self.JOB_LIMIT = 81
        # ===== THI·∫æT L·∫¨P ƒê∆Ø·ªúNG D·∫™N =====
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        scraper_dir = os.path.dirname(os.path.abspath(__file__))

        self.csv_output_dir = os.path.join(project_root, "dataset")
        os.makedirs(self.csv_output_dir, exist_ok=True)
        
        # S·ª≠ d·ª•ng chung file log v√† history cho to√†n b·ªô CareerLink
        self.log_file = os.path.join(scraper_dir, "CareerLink.log")
        self.id_history_file = os.path.join(scraper_dir, "CareerLink_id_history.txt")

        # <--- S·ª¨A: Th√™m 2 c·ªôt m·ªõi v√†o Header
        self.CSV_HEADER = [
            "title", "work_location", "salary", "experience", "level", "work_form", "company_name", "company_link",
            "company_size", "gender", "education", "requirement", "job_description", "benefits",
            "post_date", "deadline", "link", "source_web",
            "scraped_at", "transform_status" # <--- 2 c·ªôt m·ªõi
        ]

        # <--- TH√äM M·ªöI: Thi·∫øt l·∫≠p logger
        self._setup_logging()
        # T·∫°o 1 logger con ri√™ng cho category n√†y (v√≠ d·ª•: 'CareerLink.IT-Software')
        self.logger = logging.getLogger(f"{self.SOURCE_WEB}.{self.category_name}") # <--- TH√äM M·ªöI

    def _setup_logging(self): # <--- TH√äM M·ªöI: H√†m thi·∫øt l·∫≠p logging
        """
        C·∫•u h√¨nh base logger 'CareerLink'. 
        Ch·ªâ th√™m handler N·∫æU n√≥ ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p (tr√°nh l·∫∑p log khi t·∫°o nhi·ªÅu instance).
        """
        base_logger = logging.getLogger(self.SOURCE_WEB) # Logger g·ªëc l√† 'CareerLink'
        base_logger.setLevel(logging.INFO)

        # Ch·ªâ th√™m handler n·∫øu logger n√†y ch∆∞a c√≥
        if not base_logger.hasHandlers():
            # ƒê·ªãnh d·∫°ng log (bao g·ªìm t√™n c·ªßa logger con)
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

            # Handler cho File
            file_handler = logging.FileHandler(self.log_file, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            file_handler.setFormatter(formatter)
            base_logger.addHandler(file_handler)

            # Handler cho Console
            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setLevel(logging.INFO)
            console_handler.setFormatter(formatter)
            base_logger.addHandler(console_handler)

    def _create_driver(self):
        """T·∫°o v√† tr·∫£ v·ªÅ m·ªôt instance c·ªßa Chrome WebDriver."""
        chrome_options = Options()
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--disable-infobars")
        chrome_options.add_argument("--start-maximized")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--headless=new")
        return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

    

    def _get_existing_ids(self):
        """ƒê·ªçc v√† tr·∫£ v·ªÅ m·ªôt set c√°c ID ƒë√£ c√†o t·ª´ tr∆∞·ªõc."""
        if not os.path.exists(self.id_history_file): return set()
        try:
            with open(self.id_history_file, 'r', encoding='utf-8') as f:
                return {line.strip() for line in f if line.strip()}
        except Exception as e:
            self.logger.error(f"L·ªói khi ƒë·ªçc file l·ªãch s·ª≠ ID {self.id_history_file}: {e}") # <--- S·ª¨A
            return set()

    def _extract_job_id_from_link(self, link):
        """Tr√≠ch xu·∫•t ID t·ª´ link job c·ªßa CareerLink."""
        if not link: return None
        match = re.search(r'/(\d+)(?=\?|$)', link)
        return match.group(1) if match else None

    def _human_like_scroll(self, driver):
        """Cu·ªôn trang m·ªôt c√°ch t·ª± nhi√™n."""
        scroll_height = driver.execute_script("return document.body.scrollHeight")
        current_position = 0
        step = random.randint(300, 600)
        while current_position < scroll_height:
            driver.execute_script(f"window.scrollTo(0, {current_position + step});")
            current_position += step
            time.sleep(random.uniform(0.3, 0.8))

    def _safe_text(self, driver, by, selector):
        """L·∫•y text c·ªßa element m·ªôt c√°ch an to√†n."""
        try:
            return driver.find_element(by, selector).text.strip()
        except:
            return ""

    def _get_max_page(self, driver, link):
        """L·∫•y s·ªë trang t·ªëi ƒëa c·ªßa m·ªôt danh m·ª•c."""
        driver.get(link)
        try:
            WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.pagination li a")))
            pages = driver.find_elements(By.CSS_SELECTOR, "ul.pagination li a")
            max_page = 1
            for p in pages:
                try: 
                    num = int(p.text.strip())
                    max_page = max(max_page, num)
                except: continue
            return max_page
        except:
            return 1

    def run(self):
        """Ph∆∞∆°ng th·ª©c ch√≠nh ƒë·ªÉ ch·∫°y to√†n b·ªô qu√° tr√¨nh c√†o d·ªØ li·ªáu."""
        self.logger.info("üöÄ B·∫Øt ƒë·∫ßu phi√™n c√†o d·ªØ li·ªáu CareerLink m·ªõi...") # <--- S·ª¨A
        
        now_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        output_file = os.path.join(self.csv_output_dir, f"CareerLink_{self.category_name}_jobs_{now_str}.csv")
        self.logger.info(f"üìÑ D·ªØ li·ªáu l·∫ßn n√†y s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o file: {os.path.basename(output_file)}") # <--- S·ª¨A
        
        with open(output_file, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(self.CSV_HEADER)

        driver = self._create_driver()
        existing_ids = self._get_existing_ids()
        self.logger.info(f"üìä ƒê√£ t√¨m th·∫•y {len(existing_ids)} ID jobs trong l·ªãch s·ª≠ chung c·ªßa CareerLink.") # <--- S·ª¨A
        
        new_jobs_to_crawl = []
        try:
            max_page = self._get_max_page(driver, self.base_url)
            self.logger.info(f"üîé Link {self.base_url} c√≥ t·ªëi ƒëa {max_page} trang.") # <--- S·ª¨A
        except Exception as e:
            self.logger.error(f"‚ùå Kh√¥ng th·ªÉ l·∫•y s·ªë trang t·ªëi ƒëa. L·ªói: {e}. D·ª´ng ch∆∞∆°ng tr√¨nh.") # <--- S·ª¨A
            driver.quit()
            return

        
        for page in range(1, max_page + 1):
            
            url = f"{self.base_url}?page={page}"
            self.logger.info(f"üîé ƒêang qu√©t trang {page}: {url}") # <--- S·ª¨A
            try:
                driver.get(url)
                time.sleep(random.uniform(2, 4))
                self._human_like_scroll(driver)
                WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a.job-link.clickable-outside")))
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Trang {page} kh√¥ng load ƒë∆∞·ª£c. B·ªè qua trang. L·ªói: {e}") # <--- S·ª¨A
                continue

            job_cards = driver.find_elements(By.CSS_SELECTOR, "a.job-link.clickable-outside")
            if not job_cards:
                self.logger.info(f"‚úÖ Web ch·ªâ c√≥ t·ªõi trang {page-1}. D·ª´ng thu th·∫≠p.") # <--- S·ª¨A
                break

            new_jobs_found_on_page = 0
            for card in job_cards:
                try:
                    link_job = card.get_attribute("href")
                    job_id = self._extract_job_id_from_link(link_job)
                    if job_id and job_id not in existing_ids:
                        new_jobs_to_crawl.append((link_job, job_id))
                        existing_ids.add(job_id)
                        new_jobs_found_on_page += 1
                except:
                    continue
            
            if new_jobs_found_on_page > 0:
                self.logger.info(f"Trang {page} ‚Üí T√¨m th·∫•y {new_jobs_found_on_page} job M·ªöI.") # <--- S·ª¨A
            else:
                self.logger.info(f"Trang {page} kh√¥ng c√≥ job n√†o m·ªõi. (Ti·∫øp t·ª•c qu√©t...)") # <--- S·ª¨A

            pause_time = random.uniform(self.PAUSE_BETWEEN_PAGES_MIN, self.PAUSE_BETWEEN_PAGES_MAX)
            self.logger.info(f"--- Ngh·ªâ {round(pause_time, 1)} gi√¢y tr∆∞·ªõc khi sang trang ti·∫øp theo ---") # <--- S·ª¨A
            time.sleep(pause_time)

        self.logger.info(f"üéâ ƒê√£ thu th·∫≠p xong. C√≥ {len(new_jobs_to_crawl)} job m·ªõi c·∫ßn c√†o chi ti·∫øt.") # <--- S·ª¨A
        
        success_count, error_count = 0, 0
        
        if not new_jobs_to_crawl:
            self.logger.info("Kh√¥ng c√≥ job m·ªõi n√†o ƒë·ªÉ c√†o. K·∫øt th√∫c.") # <--- S·ª¨A
        else:
            for idx, (link, job_id) in enumerate(new_jobs_to_crawl, 1):
                try:
                    driver.get(link)
                    time.sleep(random.uniform(2, 5))
                    self._human_like_scroll(driver)
                    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.job-detail")))
                    
                    # <--- TH√äM M·ªöI: L·∫•y th·ªùi gian c√†o
                    scraped_timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                    title = self._safe_text(driver, By.CSS_SELECTOR, "h1.job-title.mb-0")
                    work_location = self._safe_text(driver, By.XPATH, "(//div[@class='d-flex align-items-center mb-2'])[1]")
                    salary = self._safe_text(driver, By.XPATH, "(//div[@class='d-flex align-items-center mb-2'])[2]")
                    experience = self._safe_text(driver, By.XPATH, "(//div[@class='d-flex align-items-center mb-2'])[3]")
                    post_date = self._safe_text(driver, By.XPATH, "//div[@id='job-date']//div[contains(@class,'date-from')]//span[last()]")
                    deadline = self._safe_text(driver, By.XPATH, "//div[@id='job-date']//div[contains(@class,'day-expired')]//b")
                    job_description = self._safe_text(driver, By.XPATH, '//div[@id="section-job-description"]//div[@class="rich-text-content"]')
                    skills = self._safe_text(driver, By.XPATH, '//div[@id="section-job-skills"]')
                    benefits = self._safe_text(driver, By.XPATH, '//div[@id="section-job-benefits"]')
                    try:
                        company_elem = driver.find_element(By.CSS_SELECTOR, "h5.company-name-title a")
                        company_name = company_elem.get_attribute("title").strip()
                        company_link = company_elem.get_attribute("href")
                    except: company, company_link = "", ""
                    company_size = self._safe_text(driver, By.XPATH, "//i[contains(@class,'cli-users')]/following-sibling::span")
                    level = self._safe_text(driver, By.XPATH, "//div[contains(text(),'C·∫•p b·∫≠c')]/following-sibling::div")
                    education = self._safe_text(driver, By.XPATH, "//div[contains(text(),'H·ªçc v·∫•n')]/following-sibling::div")
                    gender = self._safe_text(driver, By.XPATH, "//div[contains(text(),'Gi·ªõi t√≠nh')]/following-sibling::div")
                    work_form = self._safe_text(driver, By.XPATH, "//div[contains(text(),'Lo·∫°i c√¥ng vi·ªác')]/following-sibling::div")

                    with open(output_file, "a", encoding="utf-8-sig", newline="") as f:
                        writer = csv.writer(f)
                        # <--- S·ª¨A: Th√™m 2 c·ªôt m·ªõi v√†o d√≤ng
                        writer.writerow([
                            title, work_location, salary, experience, level, work_form,
                            company_name, company_link, company_size, gender, education,
                            skills, job_description, benefits, post_date, deadline, link, self.SOURCE_WEB,
                            scraped_timestamp, 0 # <--- 2 c·ªôt m·ªõi
                        ])
                    
                    with open(self.id_history_file, "a", encoding="utf-8") as f:
                        f.write(job_id + "\n")

                    success_count += 1
                    self.logger.info(f"‚úÖ [{success_count}/{len(new_jobs_to_crawl)}] ƒê√£ c√†o v√† l∆∞u job ID {job_id}: {title[:60]}...") # <--- S·ª¨A
                    
                    #m·ªói l·∫ßn c≈©ng ch·ªâ c√†o ƒëc th√™m 81 jobs, l·ªõn h∆°n l√† l·ªói
                    if success_count >= self.JOB_LIMIT:
                        self.logger.info(f"üîî ƒê√£ ƒë·∫°t gi·ªõi h·∫°n {self.JOB_LIMIT} job th√†nh c√¥ng. D·ª´ng c√†o chi ti·∫øt.") # <--- S·ª¨A
                        break # Tho√°t kh·ªèi v√≤ng l·∫∑p c√†o chi ti·∫øt

                    if success_count % self.JOBS_PER_LONG_BREAK == 0 and success_count < len(new_jobs_to_crawl):
                        sleep_time = random.uniform(self.LONG_BREAK_DURATION_MIN, self.LONG_BREAK_DURATION_MAX)
                        self.logger.info(f"‚è∏ Ngh·ªâ d√†i sau {success_count} job... S·∫Ω ti·∫øp t·ª•c sau {round(sleep_time/60, 1)} ph√∫t.") # <--- S·ª¨A
                        time.sleep(sleep_time)
                    else:
                        time.sleep(random.uniform(self.PAUSE_BETWEEN_JOBS_MIN, self.PAUSE_BETWEEN_JOBS_MAX))
                
                except Exception as e:
                    error_count += 1
                    self.logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω link {idx}/{len(new_jobs_to_crawl)} (ID: {job_id}): {link} | {e}") # <--- S·ª¨A
                    driver.get(self.base_url)
                    time.sleep(5)
            
        driver.quit()
        
        # <--- TH√äM M·ªöI: Logic n·∫°p DB v√† d·ªçn d·∫πp file CSV ---
        if success_count > 0:
            self.logger.info(f"--- B·∫ÆT ƒê·∫¶U N·∫†P V√ÄO DATABASE ({os.path.basename(output_file)}) ---")
            load_csv_to_staging_and_cleanup(output_file, schema='staging', table_name='raw_jobs')
            self.logger.info(f"--- K·∫æT TH√öC N·∫†P V√ÄO DATABASE ---")
        elif not new_jobs_to_crawl:
            self.logger.info("Kh√¥ng c√≥ job m·ªõi, kh√¥ng c·∫ßn n·∫°p v√†o DB.")
            try:
                os.remove(output_file) # X√≥a file CSV r·ªóng (ch·ªâ c√≥ header)
                self.logger.info(f"ƒê√£ x√≥a file CSV r·ªóng: {output_file}")
            except Exception as e:
                self.logger.error(f"Kh√¥ng th·ªÉ x√≥a file r·ªóng {output_file}: {e}")
        else: # C√≥ job m·ªõi nh∆∞ng c√†o l·ªói 100%
            self.logger.warning(f"T·∫•t c·∫£ {len(new_jobs_to_crawl)} job m·ªõi ƒë·ªÅu c√†o b·ªã l·ªói. Kh√¥ng n·∫°p v√†o DB.")
            try:
                os.remove(output_file) # X√≥a file CSV r·ªóng (ch·ªâ c√≥ header)
                self.logger.info(f"ƒê√£ x√≥a file CSV r·ªóng: {output_file}")
            except Exception as e:
                self.logger.error(f"Kh√¥ng th·ªÉ x√≥a file r·ªóng {output_file}: {e}")
        # --- H·∫øt kh·ªëi code th√™m m·ªõi ---
        
        self.logger.info(f"üéâ Crawl xong - ƒê√£ l∆∞u {success_count} job M·ªöI, L·ªói: {error_count}") # <--- S·ª¨A