import selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time, random, csv, os, datetime, sys, re

# ==========================================================
# ===== C·∫§U H√åNH CHO PIPELINE (VIETNAMWORKS ?g=5) =====
# ==========================================================
# S·ªë trang S·∫º C√ÄO TH√äM cho l·∫ßn ch·∫°y k·∫ø ti·∫øp
PAGES_TO_ADD_PER_RUN = 2 
JOBS_PER_BREAK = 50
BREAK_DURATION_MIN = 120
BREAK_DURATION_MAX = 300
BATCH_SIZE = 50 # S·ªë job c√†o xong th√¨ restart driver

# --- Th√¥ng tin ri√™ng c·ªßa trang n√†y ---
TARGET_URL = "https://www.vietnamworks.com/viec-lam?g=5"
SOURCE_WEB_NAME = "VietnamWorks_g5" # T√™n ƒë·ªÉ l∆∞u v√†o c·ªôt 'source_web'

# --- C√°c Selector (L·∫•y t·ª´ code ch√∫ng ta v·ª´a test) ---
JOB_CARD_SELECTOR = "div.view_job_item.new-job-card" 
LINK_SELECTOR_INSIDE_CARD = "h2 a"
NEXT_BUTTON_XPATH = "//ul[contains(@class, 'pagination')]//li/button[text()='>']"

# ===== H√†m setup Chrome (Kh√¥ng thay ƒë·ªïi) =====
def create_driver():
    chrome_options = Options()
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument("--disable-infobars")
    chrome_options.add_argument("--start-maximized")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # chrome_options.add_argument("--headless=new") # B·∫≠t l·∫°i d√≤ng n√†y khi ch·∫°y tr√™n server
    return webdriver.Chrome(options=chrome_options)

# ===== C√°c h√†m setup file, log =====
# Th∆∞ m·ª•c g·ªëc (ƒê·ªïi t√™n ƒë·ªÉ kh√¥ng ƒë√® l√™n TopCV)
output_dir = "VietnamWorks_g5"
os.makedirs(output_dir, exist_ok=True)

# Th∆∞ m·ª•c con ƒë·ªÉ l∆∞u CSV
csv_output_dir = os.path.join(output_dir, "VNW_g5_csv")
os.makedirs(csv_output_dir, exist_ok=True)

# C√°c file qu·∫£n l√Ω tr·∫°ng th√°i
log_file = os.path.join(output_dir, "VNW_g5_log.txt")
id_history_file = os.path.join(output_dir, "id_jobhistory.txt") 
# File n√†y s·∫Ω l∆∞u s·ªë trang t·ªëi ƒëa s·∫Ω c√†o (v√≠ d·ª•: 3, 5, 7, 9...)
max_page_file = os.path.join(output_dir, "max_pages_to_crawl.txt")

# T·∫°o file CSV m·ªõi cho m·ªói l·∫ßn ch·∫°y
now_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
output_file = os.path.join(csv_output_dir, f"VNW_g5_jobs_{now_str}.csv")

# Danh s√°ch c√°c c·ªôt (HEADER) - L·∫•y theo y√™u c·∫ßu c·ªßa b·∫°n
CSV_HEADER = [
    "title", "work_location", "experience", "salary",
    "work_time", "level", "work_form", "company_name", "company_link",
    "company_size", "recruit_quantity", "education",
    "requirement", "job_description", "benefits", "deadline", "link", "source_web"
]

# Kh·ªüi t·∫°o file CSV
with open(output_file, "w", encoding="utf-8-sig", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(CSV_HEADER)

def write_log(message):
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(log_file, "a", encoding="utf-8") as log:
        log.write(f"[{now}] {message}\n")
    print(message)

# ===== C√°c h√†m x·ª≠ l√Ω ID (Kh√¥ng thay ƒë·ªïi) =====
def get_existing_ids(file_path):
    """ƒê·ªçc file l·ªãch s·ª≠ v√† tr·∫£ v·ªÅ m·ªôt SET ch·ª©a c√°c ID ƒë√£ c√†o."""
    if not os.path.exists(file_path):
        return set()
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return {line.strip() for line in f if line.strip()}
    except Exception as e:
        write_log(f"L·ªói khi ƒë·ªçc file l·ªãch s·ª≠ ID {file_path}: {e}")
        return set()

def extract_job_id_from_link(link):
    """Tr√≠ch xu·∫•t chu·ªói s·ªë ID t·ª´ cu·ªëi URL (Ho·∫°t ƒë·ªông cho c·∫£ TopCV v√† VNW)."""
    if not link:
        return None
    match = re.search(r'/(\d+)\.html', link)
    if match:
        return match.group(1)
    # Th√™m d·ª± ph√≤ng cho link VNW c√≥ ?jv=
    match_jv = re.search(r'-(\d+)-jv', link)
    if match_jv:
        return match_jv.group(1)
    write_log(f"WARNING: Kh√¥ng th·ªÉ tr√≠ch xu·∫•t ID t·ª´ link: {link}")
    return None

# ===== H√†m h·ªó tr·ª£ l·∫•y text an to√†n =====
def get_safe(driver, selector, by=By.CSS_SELECTOR):
    """L·∫•y .text c·ªßa element, tr·∫£ v·ªÅ "" n·∫øu kh√¥ng t√¨m th·∫•y."""
    try:
        return driver.find_element(by, selector).text.strip()
    except:
        return ""

def get_safe_attr(driver, selector, attribute, by=By.CSS_SELECTOR):
    """L·∫•y attribute c·ªßa element, tr·∫£ v·ªÅ "" n·∫øu kh√¥ng t√¨m th·∫•y."""
    try:
        return driver.find_element(by, selector).get_attribute(attribute)
    except:
        return ""


# ==========================================================
# ===== B·∫ÆT ƒê·∫¶U CH∆Ø∆†NG TR√åNH CH√çNH =====
# ==========================================================
start_time = time.time()
write_log(f"üöÄ B·∫Øt ƒë·∫ßu phi√™n c√†o d·ªØ li·ªáu {SOURCE_WEB_NAME} m·ªõi...")
write_log(f"üìÑ D·ªØ li·ªáu l·∫ßn n√†y s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o file: {os.path.basename(output_file)}")

# Logic ƒë·ªçc v√† qu·∫£n l√Ω max_page_file.txt (Gi·ªëng TopCV)
try:
    if not os.path.exists(max_page_file):
        # L·∫ßn ƒë·∫ßu ti√™n ch·∫°y, ƒë·∫∑t s·ªë trang c√†o = s·ªë trang th√™m m·ªói l·∫ßn (v√≠ d·ª•: 2)
        max_pages_to_crawl_this_run = PAGES_TO_ADD_PER_RUN 
        with open(max_page_file, 'w') as f:
            f.write(str(max_pages_to_crawl_this_run))
        write_log(f"File {max_page_file} kh√¥ng t·ªìn t·∫°i. T·∫°o m·ªõi v√† ƒë·∫∑t s·ªë trang c√†o l√† {max_pages_to_crawl_this_run}.")
    else:
        with open(max_page_file, 'r') as f:
            content = f.readline().strip()
            if content and content.isdigit():
                max_pages_to_crawl_this_run = int(content)
            else:
                max_pages_to_crawl_this_run = PAGES_TO_ADD_PER_RUN
                write_log(f"N·ªôi dung file {max_page_file} kh√¥ng h·ª£p l·ªá. ƒê·∫∑t l·∫°i s·ªë trang c√†o l√† {max_pages_to_crawl_this_run}.")
except Exception as e:
    max_pages_to_crawl_this_run = PAGES_TO_ADD_PER_RUN
    write_log(f"L·ªói khi ƒë·ªçc file {max_page_file}: {e}. ƒê·∫∑t l·∫°i s·ªë trang c√†o l√† {max_pages_to_crawl_this_run}.")

write_log(f"üìå L·∫ßn n√†y s·∫Ω c√†o t·ªëi ƒëa {max_pages_to_crawl_this_run} trang.")

driver = create_driver()

# T·∫£i c√°c ID ƒë√£ c√†o v√†o m·ªôt SET ƒë·ªÉ ki·ªÉm tra nhanh
existing_ids = get_existing_ids(id_history_file)
write_log(f"üìä ƒê√£ t√¨m th·∫•y {len(existing_ids)} ID jobs trong l·ªãch s·ª≠.")

# =========================================================================
# B1: (ƒê√£ thay ƒë·ªïi) Thu th·∫≠p link v√† ID M·ªöI b·∫±ng c√°ch chuy·ªÉn trang
# =========================================================================
new_jobs_to_crawl = [] # S·∫Ω ch·ª©a c√°c tuple (link, job_id)
stop_crawling = False
current_page = 1

try:
    write_log(f"üîé ƒêang truy c·∫≠p trang: {TARGET_URL}")
    driver.get(TARGET_URL)
    
    # === V√≤ng l·∫∑p c√†o nhi·ªÅu trang (L·∫•y t·ª´ code test) ===
    while current_page <= max_pages_to_crawl_this_run:
        if stop_crawling:
            break
            
        write_log(f"\n=========================================")
        write_log(f"üîé B·∫ÆT ƒê·∫¶U QU√âT TRANG {current_page} üîé")
        
        # 1. Ch·ªù job card xu·∫•t hi·ªán
        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, JOB_CARD_SELECTOR))
            )
        except TimeoutException:
            write_log("‚ùå H·∫øt gi·ªù ch·ªù, kh√¥ng th·∫•y job card. D·ª´ng l·∫°i.")
            break 

        # 2. L·∫•y t·∫•t c·∫£ job card
        job_cards = driver.find_elements(By.CSS_SELECTOR, JOB_CARD_SELECTOR)
        if not job_cards:
            write_log("‚úÖ Kh√¥ng t√¨m th·∫•y job card n√†o. D·ª´ng.")
            break

        # 3. L·∫∑p qua c√°c card v√† KI·ªÇM TRA ID (Logic c·ªßa TopCV)
        new_jobs_found_on_page = 0
        for card in job_cards:
            try:
                link_element = card.find_element(By.CSS_SELECTOR, LINK_SELECTOR_INSIDE_CARD)
                link = link_element.get_attribute("href")
                job_id = extract_job_id_from_link(link)

                if job_id and job_id not in existing_ids:
                    new_jobs_to_crawl.append((link, job_id))
                    existing_ids.add(job_id) # Th√™m ngay v√†o set ƒë·ªÉ tr√°nh tr√πng l·∫∑p
                    new_jobs_found_on_page += 1
            except Exception as e:
                write_log(f"L·ªói nh·ªè khi l·∫•y link/ID 1 card: {e}")
                continue
                
        write_log(f"Trang {current_page} ‚Üí T√¨m th·∫•y {new_jobs_found_on_page} job M·ªöI.")

        # 4. Logic "D·ª´ng th√¥ng minh" (C·ªßa TopCV)
        # N·∫øu trang n√†y kh√¥ng c√≥ job m·ªõi (v√† ƒë√¢y kh√¥ng ph·∫£i trang 1), th√¨ d·ª´ng
        if new_jobs_found_on_page == 0 and current_page > 1:
            write_log(f"‚úÖ Trang {current_page} kh√¥ng c√≥ job n√†o m·ªõi. D·ª´ng thu th·∫≠p link.")
            stop_crawling = True
            break
            
        # 5. T√¨m v√† Click n√∫t "Next" (Logic c·ªßa VNW test)
        try:
            next_button = driver.find_element(By.XPATH, NEXT_BUTTON_XPATH)
            
            if next_button.is_enabled():
                write_log("üñ±Ô∏è ƒêang click v√†o n√∫t 'Next'...")
                next_button.click()
                current_page += 1 # TƒÉng s·ªë trang l√™n
                write_log(" ¬† -> Ch·ªù trang m·ªõi t·∫£i (3 gi√¢y)...")
                time.sleep(random.uniform(3, 5))
            else:
                write_log("‚ùå N√∫t 'Next' ƒë√£ b·ªã m·ªù. ƒê√¢y l√† trang cu·ªëi. D·ª´ng.")
                break 

        except NoSuchElementException:
            write_log("‚ùå Kh√¥ng t√¨m th·∫•y n√∫t 'Next'. ƒê√¢y l√† trang cu·ªëi. D·ª´ng.")
            break # Tho√°t kh·ªèi v√≤ng l·∫∑p while

except Exception as e:
    write_log(f"L·ªói nghi√™m tr·ªçng ·ªü Giai ƒëo·∫°n 1: {e}")
    
write_log(f"üéâ ƒê√£ thu th·∫≠p xong. C√≥ {len(new_jobs_to_crawl)} job m·ªõi c·∫ßn c√†o chi ti·∫øt.")


# ===============================================
# B2: V√†o t·ª´ng link l·∫•y chi ti·∫øt v√† l∆∞u tr·ªØ
# ===============================================
success_count, error_count = 0, 0
if not new_jobs_to_crawl:
    write_log("Kh√¥ng c√≥ job m·ªõi n√†o ƒë·ªÉ c√†o. K·∫øt th√∫c.")
else:
    write_log("--- B·∫ÆT ƒê·∫¶U C√ÄO CHI TI·∫æT ---")

    # =========================================================================
    # ===== THAY ƒê·ªîI 1: GI·ªöI H·∫†N CH·∫†Y 5 JOBS ƒê·ªÇ TEST =====
    # Ch√∫ng ta th√™m [:5] ƒë·ªÉ ch·ªâ l·∫•y 5 ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n
    # =========================================================================
    write_log(f"--- !!! CH·∫æ ƒê·ªò TEST: CH·ªà L·∫§Y 5 JOBS ƒê·∫¶U TI√äN T·ª™NG {len(new_jobs_to_crawl)} JOBS M·ªöI T√åM TH·∫§Y ---")
    for idx, (link, job_id) in enumerate(new_jobs_to_crawl[:5], 1):
        try:
            driver.get(link)
            # Ch·ªù m·ªôt element ƒë·∫∑c tr∆∞ng c·ªßa trang chi ti·∫øt
            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, "h1.job-title")))
            time.sleep(random.uniform(2, 4))
            
            
            
            write_log(f"--- ƒêang c√†o ID: {job_id} ---")

            title = get_safe(driver, "h1.title")
            try:
                xpath_diadiem = "//h2[contains(text(), 'ƒê·ªãa ƒëi·ªÉm l√†m vi·ªác')]/following-sibling::div//p[@name='paragraph']"
                work_location = driver.find_element(By.XPATH, xpath_diadiem).text.strip()
            except:
                work_location = ""
            experience = get_safe(driver, "span.experience-value")
            salary = get_safe(driver, "span.salary-value")
            work_time = "" # VNW kh√¥ng c√≥ tr∆∞·ªùng n√†y
            level = get_safe(driver, "span.level-value")
            work_form = "" # VNW kh√¥ng c√≥ tr∆∞·ªùng n√†y
            company_name = get_safe(driver, "div.company-name")
            company_link = get_safe_attr(driver, "a.company-logo-wrapper", "href")
            company_size = get_safe(driver, "span.company-size-value")
            recruit_quantity = "" # VNW kh√¥ng c√≥ tr∆∞·ªùng n√†y
            education = "" # VNW kh√¥ng c√≥ tr∆∞·ªùng n√†y
            
            # X·ª≠ l√Ω 3 kh·ªëi text l·ªõn
            requirement = ""
            job_description = ""
            benefits = ""
            try:
                full_text = driver.find_element(By.CSS_SELECTOR, "div.job-description").text
                # T√°ch M√¥ T·∫£ CV
                desc_parts = re.split(r'(Y√™u C·∫ßu C√¥ng Vi·ªác|Y√™u C·∫ßu ·ª®ng Vi√™n|Requirements)', full_text, maxsplit=1, flags=re.IGNORECASE | re.MULTILINE)
                job_description = desc_parts[0].strip()
                
                if len(desc_parts) > 1:
                    remaining_text = desc_parts[2]
                    # T√°ch Y√™u C·∫ßu v√† Quy·ªÅn L·ª£i
                    req_parts = re.split(r'(Quy·ªÅn L·ª£i|Benefits|Ph√∫c L·ª£i)', remaining_text, maxsplit=1, flags=re.IGNORECASE | re.MULTILINE)
                    requirement = req_parts[0].strip()
                    if len(req_parts) > 1:
                        benefits = req_parts[2].strip()
                
                # Fallback n·∫øu kh√¥ng t√°ch ƒë∆∞·ª£c
                if not job_description and not requirement and not benefits:
                    job_description = full_text
            except Exception:
                job_description = get_safe(driver, "div.job-description") # Fallback
            
            deadline = get_safe(driver, "span.expiration-date-value")
            link = link # ƒê√£ c√≥ s·∫µn
            source_web = SOURCE_WEB_NAME # ƒê√£ c√≥ s·∫µn
            
            
            # D·ªØ li·ªáu ƒë∆∞·ª£c s·∫Øp x·∫øp theo ƒë√∫ng th·ª© t·ª± c·ªßa CSV_HEADER
            job_data = [
                title, work_location, experience, salary,
                work_time, level, work_form, company_name, company_link,
                company_size, recruit_quantity, education,
                requirement, job_description, benefits, deadline, link, source_web
            ]
            
            # Ghi v√†o file CSV c·ªßa l·∫ßn ch·∫°y n√†y
            with open(output_file, "a", encoding="utf-8-sig", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(job_data)

            # Ghi ID v·ª´a c√†o th√†nh c√¥ng v√†o file l·ªãch s·ª≠
            with open(id_history_file, "a", encoding="utf-8") as f:
                f.write(job_id + "\n")

            success_count += 1
            write_log(f"‚úÖ [{success_count}/{len(new_jobs_to_crawl[:5])}] ƒê√£ c√†o v√† l∆∞u job ID {job_id}: {title}")
            
            # Logic ngh·ªâ v√† restart driver (Gi·ªØ nguy√™n c·ªßa TopCV)
            # (S·∫Ω kh√¥ng ch·∫°y n·∫øu ch·ªâ test 5 jobs, nh∆∞ng ƒë·ªÉ ƒë√≥ kh√¥ng sao)
            if success_count % JOBS_PER_BREAK == 0 and success_count < len(new_jobs_to_crawl):
                pause_time = random.uniform(BREAK_DURATION_MIN, BREAK_DURATION_MAX)
                write_log(f"‚è∏ ƒê√£ c√†o {success_count} job. T·∫°m ngh·ªâ {round(pause_time/60, 1)} ph√∫t...")
                time.sleep(pause_time)
            
            if idx % BATCH_SIZE == 0 and idx < len(new_jobs_to_crawl):
                write_log("üîÑ Kh·ªüi ƒë·ªông l·∫°i tr√¨nh duy·ªát...")
                driver.quit()
                time.sleep(random.uniform(20, 40))
                driver = create_driver()

        except Exception as e:
            error_count += 1
            write_log(f"‚ùå L·ªói khi x·ª≠ l√Ω link {idx}/{len(new_jobs_to_crawl[:5])} (ID: {job_id}): {link} | {e}")

driver.quit()

# ==========================================================
# ===== K·∫æT TH√öC V√Ä C·∫¨P NH·∫¨T FILE ƒê·∫æM TRANG =====
# ==========================================================

# =========================================================================
# ===== THAY ƒê·ªîI 2: V√î HI·ªÜU H√ìA C·∫¨P NH·∫¨T FILE ƒê·∫æM TRANG =====
# Ch√∫ng ta kh√¥ng mu·ªën l·∫ßn ch·∫°y TEST n√†y ·∫£nh h∆∞·ªüng ƒë·∫øn l·∫ßn ch·∫°y TH·∫¨T
# =========================================================================
# new_max_page = max_pages_to_crawl_this_run + PAGES_TO_ADD_PER_RUN
# try:
#     with open(max_page_file, "w") as f:
#         f.write(str(new_max_page))
#     write_log(f"üîÑ ƒê√£ c·∫≠p nh·∫≠t {max_page_file} cho l·∫ßn ch·∫°y ti·∫øp theo: {new_max_page}")
# except Exception as e:
#     write_log(f"‚ùå Kh√¥ng th·ªÉ c·∫≠p nh·∫≠t file {max_page_file}: {e}")

write_log("--- CH·∫æ ƒê·ªò TEST: ƒê√£ b·ªè qua b∆∞·ªõc c·∫≠p nh·∫≠t file ƒë·∫øm trang ---")

end_time = time.time()
total_minutes = round((end_time - start_time) / 60, 2)
write_log(f"üèÅ (Test) Crawl xong trong {total_minutes} ph√∫t - ƒê√£ l∆∞u {success_count} job M·ªöI, L·ªói: {error_count}")